install.packages("pROC")
#     library(pROC)
#
#     install.packages("pROC")
#     library(pROC)
#
# Read in the dataset
players <- read.table("players.txt", header = T, sep = ",")
# Get the summary statistics of the data
summary(players)
players$id <- NULL
names(players)
summary(players)
learn <- players[players$lastseason <= 1999,]
test <- players[players$firstseason > 1999,]
learn$firstseason <- NULL
learn$lastseason <- NULL
learn$firstseason <- NULL
learn$lastseason <- NULL
test$firstseason <- NULL
test$lastseason <- NULL
nrow(learn)
table(learn$position)
nrow(test)
table(test$position)
# The majority class is the class with the highest number of training examples
which.max(table(learn$position))
majority.class <- names(which.max(table(learn$position)))
majority.class
# The majority classifier classifies all test instances into the majority class.
# The accuracy of the majority class
sum(test$position == majority.class) / length(test$position)
# load the "rpart" package that implements decision trees in R
library(rpart)
# Fit a decision tree
dt <- rpart(position ~ ., data = learn)
# Fit a decision tree
dt <- rpart(position ~ ., data = learn)
# The majority classifier classifies all test instances into the majority class.
# The accuracy of the majority class
sum(test$position == majority.class) / length(test$position)
nrow(learn)
table(learn$position)
nrow(test)
table(test$position)
# The majority class is the class with the highest number of training examples
which.max(table(learn$position))
majority.class <- names(which.max(table(learn$position)))
majority.class
# The majority classifier classifies all test instances into the majority class.
# The accuracy of the majority class
sum(test$position == majority.class) / length(test$position)
# Fit a decision tree
dt <- rpart(position ~ ., data = learn)
# The content of the dt object
dt
# A graphical representation of our tree
plot(dt)
text(dt, pretty = 1)
# Observed values in the test samples (the ground truth)
observed <- test$position
observed
predicted <- predict(dt, test, type = "class")
predicted
# confusion matrix
t <- table(observed, predicted)
t
# The classification accuracy is the fraction of correctly classified test instances
sum(diag(t)) / sum(t)
# Let's write a function that calculates the classification accuracy
CA <- function(observed, predicted)
{
t <- table(observed, predicted)
sum(diag(t)) / sum(t)
}
# Function call
CA(observed, predicted)
# Function call
CA(observed, predicted)
predMat <- predict(dt, test, type = "prob")
predMat
# A matrix of the observed class probabilities (the ground truth)
# (the real class has the probability of 1, others have 0)
obsMat <- model.matrix( ~ position-1, test)
obsMat
# IMPORTANT: check whether the columns refer to the same attributes
# (if not, change the order of columns)
colnames(predMat)
colnames(obsMat)
predMat.head()
predMat
predMat[:2]
predMat[2]
# A matrix of the observed class probabilities (the ground truth)
# (the real class has the probability of 1, others have 0)
obsMat <- model.matrix( ~ position-1, test)
obsMat
# IMPORTANT: check whether the columns refer to the same attributes
# (if not, change the order of columns)
colnames(predMat)
colnames(obsMat)
# The Brier score
brier.score <- function(observedMatrix, predictedMatrix)
{
sum((observedMatrix - predictedMatrix) ^ 2) / nrow(predictedMatrix)
}
brier.score(obsMat, predMat)
# Get the players who have attempted at least one free throw during the career.
bin.players <- players[players$fta > 0,]
# Free-throw success rate
rate <- bin.players$ftm / bin.players$fta
# Create a discrete attribute "ftexpert" that will be our target variable.
ftexpert <- vector()
ftexpert[rate >= 0.8] <- "YES"
ftexpert[rate < 0.8] <- "NO"
bin.players$ftexpert <- as.factor(ftexpert)
# Remove the "fta" and "ftm" attributes.
bin.players$fta <- NULL
bin.players$ftm <- NULL
summary(bin.players)
# Split the data into two disjoint sets, one for training and one for testing.
bin.learn <- bin.players[1:1500,]
bin.test <- bin.players[-(1:1500),]
# Inspect the distribution of the class values in the data sets.
table(bin.learn$ftexpert)
table(bin.test$ftexpert)
# Train a decision tree
dt2 <- rpart(ftexpert ~ ., data = bin.learn)
plot(dt2)
text(dt2, pretty = 1)
bin.observed <- bin.test$ftexpert
bin.predicted <- predict(dt2, bin.test, type="class")
table(bin.observed, bin.predicted)
# Classification accuracy of the decision tree
CA(bin.observed, bin.predicted)
# Exercise -> majority is important!
table(bin.observed)
sum(bin.observed == "NO") / length(bin.observed)
# The sensitivity of a model (recall)
Sensitivity <- function(observed, predicted, pos.class)
{
t <- table(observed, predicted)
t[pos.class, pos.class] / sum(t[pos.class,])
}
# The specificity of a model
Specificity <- function(observed, predicted, pos.class)
{
t <- table(observed, predicted)
# identify the negative class name
neg.class <- which(row.names(t) != pos.class)
t[neg.class, neg.class] / sum(t[neg.class,])
}
Sensitivity(bin.observed, bin.predicted, "YES")
Specificity(bin.observed, bin.predicted, "YES")
library(pROC)
library(ggplot2)
library(dplyr)
library(ggrepel) # For nicer ROC visualization
bin.predMat <- predict(dt2, bin.test, type = "prob")
# Remember, we treat the value "YES" as the positive class
rocobj <- roc(bin.observed, bin.predMat[,"YES"])
plot(rocobj)
# This part below is optional -> nicer ROC
tmpDf <- data.frame(rocobj$sensitivities, 1-rocobj$specificities, rocobj$thresholds)
colnames(tmpDf) <- c("Sensitivity","Specificity", "Thresholds")
tmpDf %>% ggplot(aes(x=Specificity,y=Sensitivity,color=Thresholds))+
geom_point()+
geom_line()+
geom_abline(intercept = 0, slope = 1, alpha=0.2)+
theme_classic()+
ylab("Sensitivity (TPR)")+
xlab("1-Specificity (FPR)")+
geom_label_repel(aes(label = round(Thresholds,2)),
box.padding   = 0.35,
point.padding = 0.5,
segment.color = 'grey50')
names(rocobj)
cutoffs <- rocobj$thresholds
cutoffs
# identify the best cut-off value (for example, by minimazing the distance from the point (0,1))
tp = rocobj$sensitivities
fp = 1 - rocobj$specificities
dist <- (1-tp)^2 + fp^2
dist
which.min(dist)
best.cutoff <- cutoffs[which.min(dist)]
best.cutoff
# the selected cut-off value has impact on the model's performance
predicted.label <- factor(ifelse(bin.predMat[,"YES"] >= best.cutoff, "YES", "NO"))
table(bin.observed, predicted.label)
CA(bin.observed, predicted.label)
Sensitivity(bin.observed, predicted.label, "YES")
Specificity(bin.observed, predicted.label, "YES")
source("~/OneDrive/grive/faks/3. letnik/1. semester/is/vaje/vaje04/lab4_code.R")
